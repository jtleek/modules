
Now that we've covered a lot of the 
preliminaries, we can dive into the fun 
part of data analysis, and we're going to 
start by looking at the structure of a 
data analysis, where we will define the 
different components that form an 
analysis, and how they interact with each 
other. 
So here is the list of steps in a data 
analysis. 
You start by defining the question. 
This is a scientific or business question 
that you would like to answer with data. 
Then you define the ideal data set. 
This data set is the data set that you 
would collect if time and money were no 
object. 
Then you determine what data you can 
access. 
I have yet to run into the case where 
that data is the ideal data set. 
Then you obtain the data and you clean it 
up so that you can perform analysis. 
Then you perform an exploratory data 
analysis to understand the quirks of the 
particular data set that you've 
downloaded, and then you perform a 
statistical prediction or modeling to 
answer the question that you're 
interested in. 
The next step is to interpret the results 
so that you can tell people in plain 
language what the statistical models or 
predictions mean. 
You should also challenge the results so 
that you can explain to people what the 
potential failings of the model are and 
so that they are able to make decisions 
on the basis of your analysis. 
Then you synthesize and write up all the 
results from your analysis. 
Here it's important to know that you 
don't include every single step that you 
might have performed. 
You use this synthesis to tell a story 
about using the data to answer the 
question. 
Finally, you create reproducible code so 
that you can share your analysis with 
other people. 
In this first part, we're going to cover 
the steps from defining the question to 
Cleaning the data. 
The second part will cover all of the 
remaining steps. 
So this quote is the perfect description 
of the key challenge data analysis, it 
comes from Dan Myer who at the time was a 
high school mathematics teacher, who was 
teaching word problems to his students. 
He said, ask yourselves, what problem 
have you solved ever that was worth 
solving where you knew all of the given 
information in advance, 
where you didn't have a surplus of 
information and have to filter it out, or 
you didn't have insufficient information 
and have to go get some. 
This is almost always true in data 
analysis, we almost always have either 
too much information, too many variables, 
measured in too many different ways, or 
not enough information, not the right 
variables measured in the right ways. 
And the goal is to be able to filter or 
find all the information that you need, 
and put the relationship together to be 
able to tell a story and answer the 
question that you're interested in. 
So the first step in a data analysis is 
defining a question. 
And there are 4 parts to defining the 
question. 
There's the scientific or business 
question that you might be interested in. 
There's the type of data that you 
collected to be able to answer that 
question. There's the applied statistics, 
or the data analysis concepts that you've 
used to answer the question, and finally 
there might be some more formal 
theoretical description of the analysis 
that you've performed to try to 
understand the relationships between 
variables at a higher more abstract 
level. 
Combining just applied statistics and 
theoretical, or statistics and 
mathematics leads to statistical method 
development. 
That's not what we'll be covering in this 
class. 
Combining data, plus applied statistics 
but without any knowledge of a scientific 
question or a business application is the 
danger zone. 
This is wear you can be led astray, 
because you aren't grounded in a concrete 
concept or a particular analysis that 
That you want to address. 
This class will focus on proper data 
analyses, where you start with a 
scientific question, even if that 
question is exploratory, collect some 
data to answer that question and then 
apply statistics to be able to answer the 
question and tell the story with the 
data. 
Here's an example of a question. 
So you might start with a general 
question such as, can I automatically 
detect emails that are spam from those 
that are not? So, this question can be 
answered in a lot of different ways, 
and then there's a lot of different data 
that you could collect. 
You could collect the ISP of who is 
sending the emails to you. 
You could also collect information about 
what time the emails arrived, 
or you might collect information about 
whether it has an attachment or not. 
Here we're going to make the question a 
little more concrete, 
and this is usually what you do in a data 
analysis. 
Here we're going to ask if we can use 
quantitative characteristics about the 
text of the emails themselves to classify 
them as spam, or the opposite of spam, a 
message that we care about, called ham. 
So now we need to define the ideal data 
set. 
The ideal data set may depend a little 
bit upon your goal. 
If your goal is descriptive, you might 
want a whole population of objects. 
If the goal is exploratory, you might 
want a random sample or a whole 
population where many variables are 
measured, so you can explore the 
relationships between those variables. 
If the goal is the in- inferential 
analysis or question, then you want the 
right prop-, population, and and you want 
to be able to sample from it randomly in 
a way that you can make inferences. 
If it's a predictive analysis, like the 
analysis we're going to be talking about 
in this example, you want a training set 
and a test data set from the same 
population. 
If it's a causal analysis, you might need 
data from a randomized study to be able 
to infer the causal question without 
further, more complicated, statistical 
modeling. 
And if it's a mechanistic model, you 
might need data about all of the 
components of the system, and an apiary 
knowledge about how those components 
interact with each other. 
So here's the example today, we're trying 
to ask the question of whether Data from 
emails can be used to classify them as 
spam or ham. 
And the ideal data set might be all of 
the data on all of the emails that gmail 
has collected over the last 10 years. 
These data are store in the g-, Google 
data centers and would be great for 
analyzing our question because there 
would be an enormous amount of data, 
and they've already classified some as 
spam or ham. 
But now the next part of the data 
analysis is to determine what data you 
actually can access. 
Sometimes you can find this data for free 
on the web, 
or you can find it amongst your 
collaborators or business associates. 
Sometimes, you might need to buy the 
data. 
There are websites out there that collect 
and aggregate data on all sorts of 
questions, that might be useful to you. 
Regardless of what data you're using, be 
sure to respect the terms of use, and any 
privacy considerations associated with 
those data. 
If the data don't exist, the last resort 
is you might to generate it yourself. 
This can be an expensive process, but 
sometimes if you have a very specific 
question in mind, it's the only way to 
get the data that you actually need. 
So back to our example. 
Unfortunately, most of the Google data on 
Gmail, well I guess fortunately might be 
a better word, is behind secure doors. 
So it's impossible for us to get at that 
data. 
Since that data is so secure, we might 
have to look for a different data set 
that we can use to answer our spam versus 
ham question. 
So here's a possible solution. 
Here's a data set of emails that was 
collected by a set of individuals and put 
online at the UCI machine learning 
repository. 
Here, we have about 4600 messages, and we 
have 57 different measured variables on 
those messages that we can use to try to 
answer our question. 
So, now that we've defined the data set 
that we can access, we need to try to 
obtain the raw data. 
In this particular case, the raw data 
aren't available. 
It's only these calculated attributes. 
You need to be sure to reference the 
source of where you got the data from, 
and obtaining the data can be usually 
performed by either downloading it from 
the data, from the web, or contacting the 
right people, and polite emails tend to 
go a long way. 
If you load data from an internet source 
every time you perform an analysis, you 
need to record the URL and the time that 
the data were accessed, so that if they 
get updated, people will know that that 
change occurred and be able to adapt 
their analyses accordingly. 
So our data set, it turns out, is 
actually already been put into a posi-, 
particular R package and cleaned up so 
that it's easy to access. 
It's in the Kern Lab package. 
When you see a data set in R, the name of 
the data set might be Spam, and then the 
package it comes in is labeled here in 
curly brackets. 
So because the data has already been 
loaded in a particular R package, if we 
have that R package we're able to access 
the data directly. 
In general, the next step in the analysis 
is to clean the data. 
The raw data often needs to be processed, 
and it usually doesn't come in so nice a 
form. 
If it is pre-processed, make sure you 
understand how. 
So there might be several steps that were 
performed on the raw data to get the 
processed data. 
Hopefully those steps are documented. 
If they aren't, you might have to perform 
an analyses that to convince yourself 
that the analysis you performed isn't 
sensitive to different types of 
processing. 
You also need to understand the source of 
the data. 
Was it a census, a random sample, a 
convenient sample, or something else? It 
also may need reformatting, sub-sampling, 
or other particular operations that even 
once the data had been processed need to 
occur. 
You need to record these steps as well. 
A critical point, a critical the, here 
comes a critical juncture in a data 
analysis. You need to determine if the 
data are good enough. 
Sometimes, the data that you collected 
just aren't enough to answer the question 
that you want to answer. 
Rather than trying to answer that 
question, with data that can't be used to 
answer that question, you need to either 
quit or change data sets, and try to find 
a data set that can be used to answer the 
question. 
There's nothing worse than an analysis 
that's correctly performed on a data set 
that can't be used to answer the 
question, because that's wasted effort, 
because the question wouldn't be 
answered. 
So our clean data set can be accessed by 
installing the Kern Lab package in R. 
If you don't know how to install 
packages, see the videos on installing 
packages in the computing for data 
analysis lectures. 
Then you can load that library by typing 
library, and then in parenthesis, 
kernlab, and then typing data, and s- and 
in parenthesis, spam. 
If you look at the dimensions of spam, 
you'll see that it's 4,601 rows by 58 
columns. 
In this particular case, the first 57 
columns are different variables that 
describe the text content of those 
messages, 
and the last column labels it as spam or 
not spam. 
If you go to this web site, you're able 
to see the documentation of all the 
different variables, and that way you can 
understand how the emails were processed. 
Since we are going to perform a 
prediction analysis, we need to generate 
a test and training set. 
We'll talk a lot more about test and 
training sets when we come to the 
prediction component of the course, 
but for now, just keep in mind that we 
need 2 data sets that are representative 
of the population that we're trying to 
predict from, so that we can build a true 
predictor on one part of the, the data 
set, and see if it works on the other 
part. 
Here, we set the seed, like we talked 
about when doing simulation, so that we 
get the same random sample every time, 
and then we create a training indicator. 
This variable is either 1, if you're in, 
going to be part of the training set, or 
0, if you're part of the test set. 
We're generating binomial random 
variables of size one, so it's one head 
flip, and it's a fair coin. 
We're generating 4,601 of them, because 
there's 4,601 emails in the data set. 
If you make a table of these indicators, 
we see that we're going to assign 2287 
emails to be in the training set, and 
2314 to be in the test set. 
We can then subset the spam data set to a 
training set and a test set using a 
logical operator. 
Every time that we see that the training 
indicator is equal to 1, we assign those 
rows of the spam data set to be the 
training samples. 
Every time the training indicator is 
equal to 0, we assign those samples to be 
the test emails. 
Then we can look at the dimension of just 
the training set, and we see that the 
number of rows or the number of emails is 
22,087, just like the number of variables 
in the training set. 
In the next part, we'll consider the next 
steps in this data analysis. 

