<!DOCTYPE html>
<html>
<head>
  <title>Random Vectors</title>
  <meta charset="utf-8">
  <meta name="description" content="Random Vectors">
  <meta name="author" content="Brian Caffo, PhD">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="../../libraries/frameworks/io2012/js/slides" 
    src="../../libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "../../assets/css/custom.css">
<link rel="stylesheet" href = "../../assets/css/ribbons.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <aside class="gdbar">
        <img src="../../assets/img/bloomberg_shield.png">
      </aside>
      <hgroup class="auto-fadein">
        <h1>Random Vectors</h1>
        <h2>Mathematical Biostatistics Boot Camp</h2>
        <p>Brian Caffo, PhD<br/>Johns Hopkins Bloomberg School of Public Health</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Table of contents</h2>
  </hgroup>
  <article>
    <ol>
<li>Table of contents</li>
<li>Random vectors</li>
<li>Independence

<ul>
<li>Independent events</li>
<li>Independent random variables </li>
<li>IID random variables</li>
</ul></li>
<li>Correlation</li>
<li>Variance and correlation properties</li>
<li>Variances properties of sample means</li>
<li>The sample variance</li>
<li>Some discussion</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Random vectors</h2>
  </hgroup>
  <article>
    <ul>
<li><p>Random vectors are simply random variables collected into a vector</p>

<ul>
<li>For example if \(X\) and \(Y\) are random variables \((X, Y)\) is a random vector</li>
</ul></li>
<li><p>Joint density \(f(x, y)\) satisfies \(f > 0\) and \(\int \int f(x, y) dx dy = 1\)</p></li>
<li><p>For discrete random variables \(\sum \sum f(x, y) = 1\)</p></li>
<li><p>In this lecture we focus on <strong>independent</strong> random variables where \(f(x, y) = f(x)g(y)\)</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Independent events</h2>
  </hgroup>
  <article>
    <ul>
<li>Two events \(A\) and \(B\) are {\bf independent} if \[P(A \cap B) = P(A)P(B)\]</li>
<li>Two random variables, \(X\) and \(Y\) are independent if for any two sets \(A\) and \(B\) \[P([X \in A] \cap [Y \in B]) = P(X\in A)P(Y\in B)\]</li>
<li><p>If \(A\) is independent of \(B\) then </p>

<ul>
<li>\(A^c\) is independent of \(B\) </li>
<li>\(A\) is independent of \(B^c\)</li>
<li>\(A^c\) is independent of \(B^c\)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>What is the probability of getting two consecutive heads?</li>
<li>\(A = \{\mbox{Head on flip 1}\}\) ~ \(P(A) = .5\)</li>
<li>\(B = \{\mbox{Head on flip 2}\}\) ~ \(P(B) = .5\)</li>
<li>\(A \cap B = \{\mbox{Head on flips 1 and 2}\}\)</li>
<li>\(P(A \cap B) = P(A)P(B) = .5 \times .5 = .25\) </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>Volume 309 of {\em Science} reports on a physician who was on trial for expert testimony in a criminal trial</li>
<li>Based on an estimated prevalence of sudden infant death syndrome of \(1\) out of \(8,543\), Dr Meadow testified that that the probability of a mother having two children with SIDS was \(\left(\frac{1}{8,543}\right)^2\)</li>
<li>The mother on trial was convicted of murder</li>
<li>What was Dr Meadow&#39;s mistake(s)?</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Example: continued</h2>
  </hgroup>
  <article>
    <ul>
<li>For the purposes of this class, the principal mistake was to <em>assume</em> that the probabilities of having SIDs within a family are independent</li>
<li>That is, \(P(A_1 \cap A_2)\) is not necessarily equal to \(P(A_1)P(A_2)\)</li>
<li>Biological processes that have a believed genetic or familiar environmental component, of course, tend to be dependent within families</li>
<li>In addition, the estimated prevalence was obtained from an <em>unpublished</em> report on single cases; hence having no information about recurrence of SIDs within families</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Useful fact</h2>
  </hgroup>
  <article>
    <ul>
<li>We will use the following fact extensively in this class:</li>
</ul>

<p><em>If a collection of random variables \(X_1, X_2, \ldots, X_n\) are independent, then their joint distribution is the product of their individual densities or mass functions</em></p>

<p><em>That is, if \(f_i\) is the density for random variable \(X_i\) we have that</em>
\[
f(x_1,\ldots, x_n) = \prod_{i=1}^n f_i(x_i)
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>IID random variables</h2>
  </hgroup>
  <article>
    <ul>
<li>In the instance where \(f_1 = f_2 = \ldots = f_n\) we say that the \(X_i\) are <strong>iid</strong> for <em>independent</em> and <em>identically distributed</em></li>
<li>iid random variables are the default model for random samples</li>
<li>Many of the important theories of statistics are founded on assuming that variables are iid</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>Suppose that we flip a biased coin with success probability \(p\) \(n\) times, what is the join density of the collection of outcomes?</li>
<li>These random variables are iid with densities \(p^{x_i} (1 - p)^{1-x_i}\) </li>
<li>Therefore
\[
f(x_1,\ldots,x_n) = \prod_{i=1}^n p^{x_i} (1 - p)^{1-x_i} = p^{\sum x_i} (1 - p)^{n - \sum x_i}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Correlation</h2>
  </hgroup>
  <article>
    <ul>
<li>The <strong>covariance</strong> between two random variables \(X\) and \(Y\) is defined as 
\[
Cov(X, Y) = E[(X - \mu_x)(Y - \mu_y)] = E[X Y] - E[X]E[Y]
\]</li>
<li>The following are useful facts about covariance

<ol>
<li>\(Cov(X, Y) = Cov(Y, X)\)</li>
<li>\(Cov(X, Y)\) can be negative or positive</li>
<li>\(|Cov(X, Y)| \leq \sqrt{Var(X) Var(y)}\)</li>
</ol></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Correlation</h2>
  </hgroup>
  <article>
    <ul>
<li>The <strong>correlation</strong> between \(X\) and \(Y\) is 
\[
Cor(X, Y) = Cov(X, Y) / \sqrt{Var(X) Var(y)}
\]</li>
</ul>

<ol>
<li>\(-1 \leq Cor(X, Y) \leq 1\)</li>
<li>\(Cor(X, Y) = \pm 1\) if and only if \(X = a + bY\) for some constants \(a\) and \(b\)</li>
<li>\(Cor(X, Y)\) is unitless</li>
<li>\(X\) and \(Y\) are <strong>uncorrelated</strong> if \(Cor(X, Y) = 0\) </li>
<li> \(X\) and \(Y\) are more positively correlated, the closer \(Cor(X,Y)\) is to \(1\)</li>
<li> \(X\) and \(Y\) are more negatively correlated, the closer \(Cor(X,Y)\) is to \(-1\)</li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Some useful results</h2>
  </hgroup>
  <article>
    <ul>
<li>Let \(\{X_i\}_{i=1}^n\) be a collection of random variables

<ul>
<li>When the \(\{X_i\}\) are uncorrelated \[Var\left(\sum_{i=1}^n a_i X_i + b\right) = \sum_{i=1}^n a_i^2 Var(X_i)\]<br></li>
<li>Otherwise
\[
\begin{eqnarray*}
&   &  Var\left(\sum_{i=1}^n a_i X_i + b\right)\\
& = &\sum_{i=1}^n a_i^2 Var(X_i) + 2 \sum_{i=1}^{n-1} \sum_{j=i}^n a_i a_j Cov(X_i, X_j)
\end{eqnarray*}
\]</li>
<li>If the \(X_i\) are iid with variance \(\sigma^2\) then \(Var(\bar X) = \sigma^2/n\) and \(E[S^2] = \sigma^2\)</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Example proof</h2>
  </hgroup>
  <article>
    <p>Prove that \(Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X, Y)\)</p>

<p>\[
\hspace{-1in}  \begin{eqnarray*}
    &   & Var(X + Y) \\ \\
    & = & E[(X + Y)(X + Y)] - E[X + Y]^2 \\ \\
    & = & E[X^2 + 2XY + Y^2] - (\mu_x + \mu_y)^2 \\ \\
    & = & E[X^2 + 2XY + Y^2] - \mu_x^2 - 2\mu_x\mu_y -\mu_y^2 \\ \\
    & = & (E[X^2]-\mu_x^2) + (E[Y^2] - \mu_y^2) + 2(E[XY] - \mu_x\mu_y)\\ \\
    & = & Var(X) + Var(Y) + 2 Cov(X, Y)
  \end{eqnarray*}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Result</h2>
  </hgroup>
  <article>
    <ul>
<li>A commonly used subcase from these properties is that {\em if a collection of random variables \(\{X_i\}\) are uncorrelated}, then the variance of the sum is the sum of the variances
\[
Var\left(\sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i)
\]</li>
<li>Therefore, it is sums of variances that tend to be useful, not sums of standard deviations; that is, the standard deviation of the sum of bunch of independent random variables is the square root of the sum of the variances, not the sum of the standard deviations</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>The sample mean</h2>
  </hgroup>
  <article>
    <p>Suppose \(X_i\) are iid with variance \(\sigma^2\)</p>

<p>\[
\begin{eqnarray*}
    Var(\bar X) & = & Var \left( \frac{1}{n}\sum_{i=1}^n X_i \right)\\ \\
    & = & \frac{1}{n^2} Var\left(\sum_{i=1}^n X_i \right)\\ \\
    & = & \frac{1}{n^2} \sum_{i=1}^n Var(X_i) \\ \\
    & = & \frac{1}{n^2} \times n\sigma^2 \\ \\
    & = & \frac{\sigma^2}{n}
  \end{eqnarray*}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Some comments</h2>
  </hgroup>
  <article>
    <ul>
<li>When \(X_i\) are independent with a common variance \(Var(\bar X) = \frac{\sigma^2}{n}\)</li>
<li>\(\sigma/\sqrt{n}\) is called {\bf the standard error} of the sample mean</li>
<li>The standard error of the sample mean is the standard deviation of the distribution of the sample mean</li>
<li>\(\sigma\) is the standard deviation of the distribution of a single observation</li>
<li>Easy way to remember, the sample mean has to be less variable than a single observation, therefore its standard deviation is divided by a \(\sqrt{n}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>The sample variance</h2>
  </hgroup>
  <article>
    <ul>
<li>The {\bf sample variance} is defined as 
\[
S^2 =   \frac{\sum_{i=1}^n (X_i - \bar X)^2}{n-1} 
\]</li>
<li>The sample variance is an estimator of \(\sigma^2\)</li>
<li>The numerator has a version that&#39;s quicker for calculation
\[
\sum_{i=1}^n (X_i - \bar X)^2 = \sum_{i=1}^n X_i^2 - n \bar X^2
\]</li>
<li>The sample variance is (nearly) the mean of the squared deviations from the mean</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>The sample variance is unbiased</h2>
  </hgroup>
  <article>
    <p>\[
  \begin{eqnarray*}
    E\left[\sum_{i=1}^n (X_i - \bar X)^2\right] & = & \sum_{i=1}^n E\left[X_i^2\right] - n E\left[\bar X^2\right] \\ \\
    & = & \sum_{i=1}^n \left\{Var(X_i) + \mu^2\right\} - n \left\{Var(\bar X) + \mu^2\right\} \\ \\
    & = & \sum_{i=1}^n \left\{\sigma^2 + \mu^2\right\} - n \left\{\sigma^2 / n + \mu^2\right\} \\ \\
    & = & n \sigma^2 + n \mu ^ 2 - \sigma^2 - n \mu^2 \\ \\
    & = & (n - 1) \sigma^2
  \end{eqnarray*}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Hoping to avoid some confusion</h2>
  </hgroup>
  <article>
    <ul>
<li>Suppose \(X_i\) are iid with mean \(\mu\) and variance \(\sigma^2\)</li>
<li>\(S^2\) estimates \(\sigma^2\)</li>
<li>The calculation of \(S^2\) involves dividing by \(n-1\)</li>
<li>\(S / \sqrt{n}\) estimates \(\sigma / \sqrt{n}\) the standard error of the mean</li>
<li>\(S / \sqrt{n}\) is called the sample standard error (of the mean)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>In a study of 495 organo-lead workers, the following summaries were obtained for TBV in \(cm^3\)</li>
<li>mean = \(1151.281\)</li>
<li>sum of squared observations = \(662361978\)</li>
<li>sample sd = \(\sqrt{(662361978 - 495 \times 1151.281^2)/494} = 112.6215\)</li>
<li>estimated se of the mean = \(112.6215 / \sqrt{495} = 5.062\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="../../libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="../../libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>