<!DOCTYPE html>
<html>
<head>
  <title>Conditional Probability</title>
  <meta charset="utf-8">
  <meta name="description" content="Conditional Probability">
  <meta name="author" content="Brian Caffo, PhD">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="../../libraries/frameworks/io2012/js/slides" 
    src="../../libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "../../assets/css/custom.css">
<link rel="stylesheet" href = "../../assets/css/ribbons.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <aside class="gdbar">
        <img src="../../assets/img/bloomberg_shield.png">
      </aside>
      <hgroup class="auto-fadein">
        <h1>Conditional Probability</h1>
        <h2>Mathematical Biostatistics Boot Camp</h2>
        <p>Brian Caffo, PhD<br/>Johns Hopkins Bloomberg School of Public Health</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Conditional probability, motivation</h2>
  </hgroup>
  <article>
    <ul>
<li>The probability of getting a one when rolling a (standard) die
is usually assumed to be one sixth</li>
<li>Suppose you were given the extra information that the die roll
was an odd number (hence 1, 3 or 5)</li>
<li><em>conditional on this new information</em>, the probability of a
one is now one third</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Conditional probability, definition</h2>
  </hgroup>
  <article>
    <ul>
<li>Let \(B\) be an event so that \(P(B) > 0\)</li>
<li>Then the conditional probability of an event \(A\) given that \(B\) has occurred is
\[
P(A ~|~ B) = \frac{P(A \cap B)}{P(B)}
\]</li>
<li>Notice that if \(A\) and \(B\) are independent, then
\[
P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A)
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>Consider our die roll example</li>
<li>\(B = \{1, 3, 5\}\)</li>
<li>\(A = \{1\}\)
\[
\begin{eqnarray*}
P(\mbox{one given that roll is odd})  & = & P(A ~|~ B) \\ \\
& = & \frac{P(A \cap B)}{P(B)} \\ \\
& = & \frac{P(A)}{P(B)} \\ \\ 
& = & \frac{1/6}{3/6} = \frac{1}{3}
\end{eqnarray*}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Conditional densities and mass functions</h2>
  </hgroup>
  <article>
    <ul>
<li>Conditional densities or mass functions of one variable conditional on the value of another</li>
<li>Let \(f(x,y)\) be a bivariate density or mass function for random variables \(X\) and \(Y\)</li>
<li>Let \(f(x)\) and \(f(y)\) be the associated marginal mass function or densities disregarding the other variables
\[
f(y) = \int f(x, y)dx ~~~~\mbox{or}~~~~ f(y) = \sum_x f(x, y) dx.
\]</li>
<li>Then the <strong>conditional</strong> density or mass function {\em given that \(Y = y\)} is given by
\[
f(x ~|~ y) = f(x, y) / f(y)
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Notes</h2>
  </hgroup>
  <article>
    <ul>
<li>It is easy to see that, in the discrete case, the definition of
conditional probability is exactly as in the definition for
conditional events where $A = $ the event that \(X = x\) and $B = $
the event that \(Y = y\)</li>
<li>The continuous definition is a little harder to motivate, since
the events \(X = x\) and \(Y = y\) each have probability 0</li>
<li>However, a useful motivation can be performed by taking the
appropriate limits as follows</li>
<li>Define \(A = \{X \leq x\}\) while \(B = \{Y \in [y, y + \epsilon]\}\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Continued</h2>
  </hgroup>
  <article>
    <p>\[
\begin{eqnarray*} 
P(X \leq x ~|~ Y \in [y, y + \epsilon]) & = & P(A ~|~ B) = \frac{P(A \cap B)}{P(B)} \\ \\  \\
& = & \frac{P(X \leq x, Y \in [y, y + \epsilon])}{P(Y \in [y, y + \epsilon])} \\ \\ \\
& = & \frac{\int_{y}^{y+\epsilon}\int_{-\infty}^{x}f(x,y)dxdy}
           {\int_{y}^{y+\epsilon} f(y) dy} \\ \\ \\
& = &  \frac{\epsilon\int_{y}^{y+\epsilon}\int_{-\infty}^{x}f(x,y)dxdy}
           {\epsilon\int_{y}^{y+\epsilon} f(y) dy} 
\end{eqnarray*}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Continued</h2>
  </hgroup>
  <article>
    <p>\[
\begin{eqnarray*}
& = & \frac{\frac{\int_{-\infty}^{y+\epsilon}\int_{\infty}^{x}f(x,y)dxdy - 
                  \int_{-\infty}^{y}\int_{-\infty}^{x}f(x,y)dxdy}{\epsilon}}
{\frac{\int_{-\infty}^{y+\epsilon} f(y) dy - \int_{-\infty}^{y} f(y) dy}{\epsilon}}\\ \\ \\
& = & \frac{\frac{g_1(y + \epsilon) - g_1(y)}{\epsilon}}{\frac{g_2(y + \epsilon) - g_2(y)}{\epsilon}}
\end{eqnarray*}
\]
where
\[
g_1(y) = \int_{-\infty}^{y}\int_{-\infty}^{x}f(x,y)dxdy ~~\mbox{and}~~ 
g_2(y) = \int_{-\infty}^{y} f(y) dy.
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <ul>
<li>Notice that the limit of the numerator and denominator tends to
\(g_1'\) and \(g_2'\) as \(\epsilon\) gets smaller and smaller</li>
<li>Hence we have that the conditional distribution function is
\[
P(X \leq x ~|~ Y = y) = \frac{\int_{-\infty}^x f(x, y)dx}{f(y)}.
\]</li>
<li>Now, taking the derivative with respect to \(x\) yields the
conditional density
\[
f(x ~|~ y) = \frac{f(x, y)}{f(y)}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Geometrically</h2>
  </hgroup>
  <article>
    <ul>
<li>Geometrically, the conditional density is obtained by taking the relevant slice of the joint density and appropriately renormalizing it</li>
<li>This idea extends to any other line, or even non-linear functions</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>Let \(f(x, y) = ye^{-xy - y}\) for \(0 \leq x\) and \(0 \leq y\)</li>
<li>Then note
\[
f(y) = \int_{0}^\infty f(x, y)dx = e^{-y}\int_{0}^\infty ye^{-xy}dx = e^{-y}
\]</li>
<li>Therefore
\[
f(x~|~ y) = f(x, y) / f(y) = \frac{ ye^{-xy - y}}{e^{-y}} = ye^{-xy}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Bayes&#39; rule</h2>
  </hgroup>
  <article>
    <ul>
<li>Let \(f(x ~|~ y)\) be the conditional density or mass function for \(X\) given
that \(Y = y\)</li>
<li>Let \(f(y)\) be the marginal distribution for \(y\)</li>
<li>Then if \(y\) is continuous
\[
f(y ~|~ x) = \frac{f(x ~|~ y) f(y)}{\int f(x ~|~ t) f(t) dt}
\]</li>
<li>If \(y\) is discrete
\[
f(y ~|~ x) = \frac{f(x ~|~ y) f(y)}{\sum_t f(x ~|~ t) f(t)}  
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Notes</h2>
  </hgroup>
  <article>
    <ul>
<li>Bayes&#39; rule relates the conditional density of \(f(y ~|~ x)\) to the \(f(x ~|~ y)\) and \(f(y)\)</li>
<li>A special case of this kind relationship is for two sets \(A\) and \(B\), which yields that
\[
P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}.
\]</li>
</ul>

<p>Proof:</p>

<ul>
<li>Let \(X\) be an indicator that event \(A\) has occurred</li>
<li>Let \(Y\) be an indicator that event \(B\) has occurred</li>
<li>Plug into the discrete version of Bayes&#39; rule</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Diagnostic tests</h2>
  </hgroup>
  <article>
    <ul>
<li>Let \(+\) and \(-\) be the events that the result of a diagnostic test is positive or negative respectively</li>
<li>Let \(D\) and \(D^c\) be the event that the subject of the test has or does not have the disease respectively </li>
<li>The <strong>sensitivity</strong> is the probability that the test is positive given that the subject actually has the disease, \(P(+ ~|~ D)\)</li>
<li>The <strong>specificity</strong> is the probability that the test is negative given that the subject does not have the disease, \(P(- ~|~ D^c)\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>More definitions</h2>
  </hgroup>
  <article>
    <ul>
<li>The <strong>positive predictive value</strong> is the probability that the subject has the  disease given that the test is positive, \(P(D ~|~ +)\)</li>
<li>The <strong>negative predictive value</strong> is the probability that the subject does not have the disease given that the test is negative, \(P(D^c ~|~ -)\)</li>
<li>The <strong>prevalence of the disease</strong> is the marginal probability of disease, \(P(D)\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>More definitions</h2>
  </hgroup>
  <article>
    <ul>
<li>The <strong>diagnostic likelihood ratio of a positive test</strong>, labeled \(DLR_+\), is \(P(+ ~|~ D) / P(+ ~|~ D^c)\), which is the \[sensitivity / (1 - specificity)\]</li>
<li>The <strong>diagnostic likelihood ratio of a negative test</strong>, labeled \(DLR_-\), is \(P(- ~|~ D) / P(- ~|~ D^c)\), which is the \[(1 - sensitivity) / specificity\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>A study comparing the efficacy of HIV tests, reports on an experiment which concluded that HIV antibody tests have a sensitivity of 99.7% and a specificity of 98.5%</li>
<li>Suppose that a subject, from a population with a .1% prevalence of HIV, receives a positive test result. What is the probability that this subject has HIV?</li>
<li>Mathematically, we want \(P(D ~|~ +)\) given the sensitivity, \(P(+ ~|~ D) = .997\), the specificity, \(P(- ~|~ D^c) =.985\), and the prevalence \(P(D) = .001\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Using Bayes&#39; formula</h2>
  </hgroup>
  <article>
    <p>\[
\begin{eqnarray*}
  P(D ~|~ +) & = &\frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\ \\
 & = & \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\ \\
 & = & \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\ \\
 & = & .062
\end{eqnarray*}
\]</p>

<ul>
<li>In this population a positive test result only suggests a 6% probability that the subject has the disease </li>
<li>(The positive predictive value is 6% for this test)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>More on this example</h2>
  </hgroup>
  <article>
    <ul>
<li>The low positive predictive value is due to low prevalence of disease and the somewhat modest specificity</li>
<li>Suppose it was known that the subject was an intravenous drug user and routinely had intercourse with an HIV infected partner</li>
<li>Notice that the evidence implied by a positive test result does not change because of the prevalence of disease in the subject&#39;s population, only our interpretation of that evidence changes</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Likelihood ratios</h2>
  </hgroup>
  <article>
    <ul>
<li>Using Bayes rule, we have
\[
P(D ~|~ +) = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)} 
\]
and
\[
P(D^c ~|~ +) = \frac{P(+~|~D^c)P(D^c)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}.
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>Likelihood ratios</h2>
  </hgroup>
  <article>
    <ul>
<li>Therefore
\[
\frac{P(D ~|~ +)}{P(D^c ~|~ +)} = \frac{P(+~|~D)}{P(+~|~D^c)}\times \frac{P(D)}{P(D^c)}
\]
ie
\[
\mbox{post-test odds of }D = DLR_+\times\mbox{pre-test odds of }D
\]</li>
<li>Similarly, \(DLR_-\) relates the decrease in the odds of the
disease after a negative test result to the odds of disease prior to
the test.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h2>HIV example revisited</h2>
  </hgroup>
  <article>
    <ul>
<li>Suppose a subject has a positive HIV test</li>
<li>\(DLR_+ = .997 / (1 - .985) \approx 66\)</li>
<li>The result of the positive test is that the odds of disease is now 66 times the pretest odds</li>
<li>Or, equivalently, the hypothesis of disease is 66 times more supported by the data than the hypothesis of no disease</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h2>HIV example revisited</h2>
  </hgroup>
  <article>
    <ul>
<li>Suppose that a subject has a negative test result </li>
<li>\(DLR_- = (1 - .997) / .985  \approx .003\)</li>
<li>Therefore, the post-test odds of disease is now \(.3\%\) of the pretest odds given the negative test.</li>
<li>Or, the hypothesis of disease is supported \(.003\) times that of the hypothesis of absence of disease given the negative test result</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="../../libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="../../libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>