<!DOCTYPE html>
<html>
<head>
  <title>A trip to Asymptopia</title>
  <meta charset="utf-8">
  <meta name="description" content="A trip to Asymptopia">
  <meta name="author" content="Brian Caffo, Jeff Leek, Roger Peng">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="../../libraries/frameworks/io2012/js/slides" 
    src="../../libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
    <link rel="stylesheet" href = "../../assets/css/custom.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.BACKUP.546.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.BASE.546.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.LOCAL.546.css">
<link rel="stylesheet" href = "../../assets/css/custom.css.orig">
<link rel="stylesheet" href = "../../assets/css/custom.css.REMOTE.546.css">
<link rel="stylesheet" href = "../../assets/css/ribbons.css">

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
    <!-- END LOGO SLIDE -->
    

    <!-- TITLE SLIDE -->
    <!-- Should I move this to a Local Layout File? -->
    <slide class="title-slide segue nobackground">
      <aside class="gdbar">
        <img src="../../assets/img/bloomberg_shield.png">
      </aside>
      <hgroup class="auto-fadein">
        <h1>A trip to Asymptopia</h1>
        <h2>Statistical Inference</h2>
        <p>Brian Caffo, Jeff Leek, Roger Peng<br/>Johns Hopkins Bloomberg School of Public Health</p>
      </hgroup>
          </slide>

    <!-- SLIDES -->
      <slide class="" id="slide-1" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    
  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Asymptotics</h2>
  </hgroup>
  <article>
    <ul>
<li>Asymptotics is the term for the behavior of statistics as the sample size (or some other relevant quantity) limits to infinity (or some other relevant number)</li>
<li>(Asymptopia is my name for the land of asymptotics, where everything works out well and there&#39;s no messes. The land of infinite data is nice that way.)</li>
<li>Asymptotics are incredibly useful for simple statistical inference and approximations </li>
<li>(Not covered in this class) Asymptotics often lead to nice understanding of procedures</li>
<li>Asymptotics generally give no assurances about finite sample performance

<ul>
<li>The kinds of asymptotics that do are orders of magnitude more difficult to work with</li>
</ul></li>
<li>Asymptotics form the basis for frequency interpretation of probabilities 
(the long run proportion of times an event occurs)</li>
<li>To understand asymptotics, we need a very basic understanding of limits.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Numerical limits</h2>
  </hgroup>
  <article>
    <ul>
<li><p>Imagine a sequence</p>

<ul>
<li>\(a_1 = .9\),</li>
<li>\(a_2 = .99\),</li>
<li>\(a_3 = .999\), ...</li>
</ul></li>
<li><p>Clearly this sequence converges to \(1\)</p></li>
<li><p>Definition of a limit: For any fixed distance we can find a point in the sequence so that the sequence is closer to the limit than that distance from that point on</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Limits of random variables</h2>
  </hgroup>
  <article>
    <ul>
<li>The problem is harder for random variables</li>
<li><p>Consider \(\bar X_n\) the sample average of the first \(n\) of a collection of \(iid\) observations</p>

<ul>
<li>Example \(\bar X_n\) could be the average of the result of \(n\) coin flips (i.e. the sample proportion of heads)</li>
</ul></li>
<li><p>We say that \(\bar X_n\) {\bf converges in probability} to a limit if for any fixed distance the {\em probability} of \(\bar X_n\) being closer (further away) than that distance from the limit converges to one (zero)</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>The Law of Large Numbers</h2>
  </hgroup>
  <article>
    <ul>
<li>Establishing that a random sequence converges to a limit is hard</li>
<li>Fortunately, we have a theorem that does all the work for us, called
the <strong>Law of Large Numbers</strong></li>
<li>The law of large numbers states that if \(X_1,\ldots X_n\) are iid from a population with mean \(\mu\) and variance \(\sigma^2\) then \(\bar X_n\) converges in probability to \(\mu\)</li>
<li>(There are many variations on the LLN; we are using a particularly lazy version, my favorite kind of version)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Law of large numbers in action</h2>
  </hgroup>
  <article>
    <pre><code class="r">n &lt;- 10000; means &lt;- cumsum(rnorm(n)) / (1  : n)
plot(1 : n, means, type = &quot;l&quot;, lwd = 2, 
     frame = FALSE, ylab = &quot;cumulative means&quot;, xlab = &quot;sample size&quot;)
abline(h = 0)
</code></pre>

<div class="rimage center"><img src="fig/unnamed-chunk-1.png" title="plot of chunk unnamed-chunk-1" alt="plot of chunk unnamed-chunk-1" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Discussion</h2>
  </hgroup>
  <article>
    <ul>
<li>An estimator is <strong>consistent</strong> if it converges to what you want to estimate

<ul>
<li>Consistency is neither necessary nor sufficient for one estimator to be better than another</li>
<li>Typically, good estimators are consistent; it&#39;s not too much to ask that if we go to the trouble of collecting an infinite amount of data that we get the right answer</li>
</ul></li>
<li>The LLN basically states that the sample mean is consistent</li>
<li>The sample variance and the sample standard deviation are consistent as well</li>
<li>Recall also that the sample mean and the sample variance are unbiased as well</li>
<li>(The sample standard deviation is biased, by the way)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>The Central Limit Theorem</h2>
  </hgroup>
  <article>
    <ul>
<li>The <strong>Central Limit Theorem</strong> (CLT) is one of the most important theorems in statistics</li>
<li>For our purposes, the CLT states that the distribution of averages of iid variables, properly normalized, becomes that of a standard normal as the sample size increases</li>
<li>The CLT applies in an endless variety of settings</li>
<li>Let \(X_1,\ldots,X_n\) be a collection of iid random variables with mean \(\mu\) and variance \(\sigma^2\)</li>
<li>Let \(\bar X_n\) be their sample average</li>
<li>Then \(\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}\) has a distribution like that of a standard normal for large \(n\).</li>
<li>Remember the form
\[\frac{\bar X_n - \mu}{\sigma / \sqrt{n}} = 
\frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}.
\]</li>
<li>Usually, replacing the standard error by its estimated value doesn&#39;t change the CLT</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>Simulate a standard normal random variable by rolling \(n\) (six sided)</li>
<li>Let \(X_i\) be the outcome for die \(i\)</li>
<li>Then note that \(\mu = E[X_i] = 3.5\)</li>
<li>\(Var(X_i) = 2.92\) </li>
<li>SE \(\sqrt{2.92 / n} = 1.71 / \sqrt{n}\)</li>
<li>Standardized mean
\[
\frac{\bar X_n - 3.5}{1.71/\sqrt{n}}
\] </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Simulation of mean of \(n\) dice</h2>
  </hgroup>
  <article>
    <div class="rimage center"><img src="fig/unnamed-chunk-2.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Coin CLT</h2>
  </hgroup>
  <article>
    <ul>
<li>Let \(X_i\) be the \(0\) or \(1\) result of the \(i^{th}\) flip of a possibly unfair coin

<ul>
<li>The sample proportion, say \(\hat p\), is the average of the coin flips</li>
<li>\(E[X_i] = p\) and \(Var(X_i) = p(1-p)\)</li>
<li>Standard error of the mean is \(\sqrt{p(1-p)/n}\)</li>
<li>Then
\[
\frac{\hat p - p}{\sqrt{p(1-p)/n}}
\]
will be approximately normally distributed</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-12" style="background:;">
  <hgroup>
    
  </hgroup>
  <article>
    <div class="rimage center"><img src="fig/unnamed-chunk-3.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" class="plot" /></div>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>CLT in practice</h2>
  </hgroup>
  <article>
    <ul>
<li>In practice the CLT is mostly useful as an approximation
\[
P\left( \frac{\bar X_n - \mu}{\sigma / \sqrt{n}} \leq z \right) \approx \Phi(z).  
\]</li>
<li>Recall \(1.96\) is a good approximation to the \(.975^{th}\) quantile of the standard normal</li>
<li>Consider
\[
\begin{eqnarray*}
  .95 & \approx & P\left( -1.96 \leq \frac{\bar X_n - \mu}{\sigma / \sqrt{n}} \leq 1.96 \right)\\ \\
  & =       & P\left(\bar X_n +1.96 \sigma/\sqrt{n} \geq \mu \geq \bar X_n - 1.96\sigma/\sqrt{n} \right),\\
\end{eqnarray*}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Confidence intervals</h2>
  </hgroup>
  <article>
    <ul>
<li>Therefore, according to the CLT, the probability that the random interval \[\bar X_n \pm z_{1-\alpha/2}\sigma / \sqrt{n}\] contains \(\mu\) is approximately 95%, where \(z_{1-\alpha/2}\) is the \(1-\alpha/2\) quantile of the standard normal distribution</li>
<li>This is called a 95% <strong>confidence interval</strong> for \(\mu\)</li>
<li>We can replace the unknown \(\sigma\) with \(s\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Give a confidence interval for the average height of sons</h2>
  </hgroup>
  <article>
    <p>in Galton&#39;s data</p>

<pre><code class="r">library(UsingR);data(father.son); x &lt;- father.son$sheight
(mean(x) + c(-1, 1) * qnorm(.975) * sd(x) / sqrt(length(x))) / 12
</code></pre>

<pre><code>[1] 5.710 5.738
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Sample proportions</h2>
  </hgroup>
  <article>
    <ul>
<li>In the event that each \(X_i\) is \(0\) or \(1\) with common success probability \(p\) then \(\sigma^2 = p(1 - p)\)</li>
<li>The interval takes the form
\[
\hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}
\]</li>
<li>Replacing \(p\) by \(\hat p\) in the standard error results in what is called a Wald confidence interval for \(p\)</li>
<li>Also note that \(p(1-p) \leq 1/4\) for \(0 \leq p \leq 1\)</li>
<li>Let \(\alpha = .05\) so that \(z_{1 -\alpha/2} = 1.96 \approx 2\) then
\[
2  \sqrt{\frac{p(1 - p)}{n}} \leq 2 \sqrt{\frac{1}{4n}} = \frac{1}{\sqrt{n}} 
\]</li>
<li>Therefore \(\hat p \pm \frac{1}{\sqrt{n}}\) is a quick CI estimate for \(p\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Example</h2>
  </hgroup>
  <article>
    <ul>
<li>Your campaign advisor told you that in a random sample of 100 likely voters,
56 intent to vote for you. 

<ul>
<li>Can you relax? Do you have this race in the bag?</li>
<li>Without access to a computer or calculator, how precise is this estimate?</li>
</ul></li>
<li><code>1/sqrt(100)=.1</code> so a back of the envelope calculation gives an approximate 95% interval of <code>(0.46, 0.66)</code>

<ul>
<li>Not enough for you to relax, better go do more campaigning!</li>
</ul></li>
<li>Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.</li>
</ul>

<pre><code class="r">round(1 / sqrt(10 ^ (1 : 6)), 3)
</code></pre>

<pre><code>[1] 0.316 0.100 0.032 0.010 0.003 0.001
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>Poisson interval</h2>
  </hgroup>
  <article>
    <ul>
<li>A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?</li>
<li>\(X \sim Poisson(\lambda t)\).</li>
<li>Estimate \(\hat \lambda = X/t\)</li>
<li>\(Var(\hat \lambda) = \lambda / t\) 
\[
\frac{\hat \lambda - \lambda}{\sqrt{\hat \lambda / t}} 
= 
\frac{X - t \lambda}{\sqrt{X}} 
\rightarrow N(0,1)
\]</li>
<li>This isn&#39;t the best interval.

<ul>
<li>There are better asymptotic intervals.</li>
<li>You can get an exact CI in this case.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h3>R code</h3>
  </hgroup>
  <article>
    <pre><code class="r">x &lt;- 5; t &lt;- 94.32; lambda &lt;- x / t
round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)
</code></pre>

<pre><code>[1] 0.007 0.099
</code></pre>

<pre><code class="r">poisson.test(x, T = 94.32)$conf
</code></pre>

<pre><code>[1] 0.01721 0.12371
attr(,&quot;conf.level&quot;)
[1] 0.95
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

      <slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h2>In the regression class</h2>
  </hgroup>
  <article>
    <pre><code class="r">exp(confint(glm(x ~ 1 + offset(log(t)), family = poisson(link = log))))
</code></pre>

<pre><code>  2.5 %  97.5 % 
0.01901 0.11393 
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>

  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
<!-- Grab CDN jQuery, fall back to local if offline -->
<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
<script>window.jQuery || document.write('<script src="../../libraries/widgets/quiz/js/jquery-1.7.min.js"><\/script>')</script>
<!-- Load Javascripts for Widgets -->
<!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="../../libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
</html>